{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "NOTEBOOK ENVIRONMENT DETECTION\n",
            "======================================================================\n",
            "Platform: linux\n",
            "Current directory: /content\n",
            "\n",
            "âš  This notebook is running in a REMOTE environment (e.g., Google Colab)\n",
            "   The data files are not accessible from: /content\n",
            "\n",
            "ðŸ“‹ TO RUN THIS NOTEBOOK:\n",
            "   Option 1: Run locally in VS Code/Jupyter with local Python kernel\n",
            "   Option 2: Upload files if in Colab (but VS Code local kernel is recommended)\n",
            "\n",
            "ðŸ“ Required files (should be in notebook directory):\n",
            "   âœ— urd_Arab.dev\n",
            "   âœ— urd_Arab.devtest\n",
            "   âœ— eng_Latn.dev\n",
            "   âœ— eng_Latn.devtest\n",
            "   âœ— multi-bleu.perl\n",
            "\n",
            "ðŸ’¡ RECOMMENDED: Use 'Select Kernel' in VS Code and choose a LOCAL Python environment\n",
            "   This will allow the notebook to access files in your workspace.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# IMPORTANT: Notebook Environment Detection\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"NOTEBOOK ENVIRONMENT DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Platform: {sys.platform}\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "# This notebook is designed to work with your Urdu-English translation data\n",
        "# The data files should be in the same directory as this notebook\n",
        "\n",
        "data_files = ['urd_Arab.dev', 'urd_Arab.devtest', 'eng_Latn.dev', 'eng_Latn.devtest', 'multi-bleu.perl']\n",
        "missing_files = [f for f in data_files if not os.path.exists(f)]\n",
        "\n",
        "if not missing_files:\n",
        "    print(f\"\\nâœ“ All required files found! Ready to proceed.\")\n",
        "else:\n",
        "    print(f\"\\nâš  This notebook is running in a REMOTE environment (e.g., Google Colab)\")\n",
        "    print(f\"   The data files are not accessible from: {os.getcwd()}\")\n",
        "    print(f\"\\nðŸ“‹ TO RUN THIS NOTEBOOK:\")\n",
        "    print(f\"   Option 1: Run locally in VS Code/Jupyter with local Python kernel\")\n",
        "    print(f\"   Option 2: Upload files if in Colab (but VS Code local kernel is recommended)\")\n",
        "    print(f\"\\nðŸ“ Required files (should be in notebook directory):\")\n",
        "    for f in data_files:\n",
        "        status = \"âœ“\" if f not in missing_files else \"âœ—\"\n",
        "        print(f\"   {status} {f}\")\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ RECOMMENDED: Use 'Select Kernel' in VS Code and choose a LOCAL Python environment\")\n",
        "    print(f\"   This will allow the notebook to access files in your workspace.\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ufOFd_ALecrW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RNN with Attention-based Encoder-Decoder Model for Urdu to English Translation\n",
        "Custom LSTM Implementation (without using nn.LSTM or nn.RNN)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hLRhr3CPnrYe"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM LSTM CELL IMPLEMENTATION (No nn.LSTM or nn.RNN used)\n",
        "# ============================================================================\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom LSTM Cell implementation from scratch.\n",
        "    LSTM has 4 gates: input gate (i), forget gate (f), cell gate (g), output gate (o)\n",
        "    \n",
        "    Args:\n",
        "        input_size: Size of input features\n",
        "        hidden_size: Size of hidden state\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Weight matrices for input-to-hidden connections\n",
        "        self.W_ii = nn.Linear(input_size, hidden_size)  # Input gate\n",
        "        self.W_if = nn.Linear(input_size, hidden_size)  # Forget gate\n",
        "        self.W_ig = nn.Linear(input_size, hidden_size)  # Cell gate\n",
        "        self.W_io = nn.Linear(input_size, hidden_size)  # Output gate\n",
        "        \n",
        "        # Weight matrices for hidden-to-hidden connections\n",
        "        self.W_hi = nn.Linear(hidden_size, hidden_size)  # Input gate\n",
        "        self.W_hf = nn.Linear(hidden_size, hidden_size)  # Forget gate\n",
        "        self.W_hg = nn.Linear(hidden_size, hidden_size)  # Cell gate\n",
        "        self.W_ho = nn.Linear(hidden_size, hidden_size)  # Output gate\n",
        "        \n",
        "    def forward(self, x, hidden_state):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM cell.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_size)\n",
        "            hidden_state: Tuple of (h, c) where each is (batch_size, hidden_size)\n",
        "            \n",
        "        Returns:\n",
        "            new_h: New hidden state (batch_size, hidden_size)\n",
        "            new_c: New cell state (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        h, c = hidden_state\n",
        "        \n",
        "        # Input gate: decides what new information to store\n",
        "        i = torch.sigmoid(self.W_ii(x) + self.W_hi(h))\n",
        "        \n",
        "        # Forget gate: decides what information to discard\n",
        "        f = torch.sigmoid(self.W_if(x) + self.W_hf(h))\n",
        "        \n",
        "        # Cell gate: creates candidate values to add to cell state\n",
        "        g = torch.tanh(self.W_ig(x) + self.W_hg(h))\n",
        "        \n",
        "        # Output gate: decides what to output\n",
        "        o = torch.sigmoid(self.W_io(x) + self.W_ho(h))\n",
        "        \n",
        "        # Update cell state: forget old info and add new info\n",
        "        new_c = f * c + i * g\n",
        "        \n",
        "        # Update hidden state: output filtered cell state\n",
        "        new_h = o * torch.tanh(new_c)\n",
        "        \n",
        "        return new_h, new_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HDH9emANnuzg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM LSTM LAYER (Using our custom LSTM cells)\n",
        "# ============================================================================\n",
        "\n",
        "class CustomLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom LSTM layer that processes sequences using custom LSTM cells.\n",
        "    \n",
        "    Args:\n",
        "        input_size: Size of input features\n",
        "        hidden_size: Size of hidden state\n",
        "        num_layers: Number of stacked LSTM layers\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Create LSTM cells for each layer\n",
        "        self.lstm_cells = nn.ModuleList([\n",
        "            LSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM layer.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
        "            hidden: Optional initial hidden state tuple (h, c)\n",
        "            \n",
        "        Returns:\n",
        "            outputs: All hidden states (batch_size, seq_len, hidden_size)\n",
        "            (h, c): Final hidden and cell states\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        \n",
        "        # Initialize hidden states if not provided\n",
        "        if hidden is None:\n",
        "            h = [torch.zeros(batch_size, self.hidden_size, device=x.device) \n",
        "                 for _ in range(self.num_layers)]\n",
        "            c = [torch.zeros(batch_size, self.hidden_size, device=x.device) \n",
        "                 for _ in range(self.num_layers)]\n",
        "        else:\n",
        "            h, c = hidden\n",
        "            h = [h[i] for i in range(self.num_layers)]\n",
        "            c = [c[i] for i in range(self.num_layers)]\n",
        "        \n",
        "        outputs = []\n",
        "        \n",
        "        # Process each time step\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "            \n",
        "            # Pass through each layer\n",
        "            for layer in range(self.num_layers):\n",
        "                h[layer], c[layer] = self.lstm_cells[layer](x_t, (h[layer], c[layer]))\n",
        "                x_t = h[layer]  # Output of current layer is input to next layer\n",
        "            \n",
        "            outputs.append(h[-1])  # Collect output from last layer\n",
        "        \n",
        "        # Stack outputs along sequence dimension\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        \n",
        "        # Stack hidden and cell states\n",
        "        h_final = torch.stack(h, dim=0)\n",
        "        c_final = torch.stack(c, dim=0)\n",
        "        \n",
        "        return outputs, (h_final, c_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5iXORY_Xnzh8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENCODER with Custom LSTM\n",
        "# ============================================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder that uses custom LSTM to encode source sequences.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of source vocabulary\n",
        "        embed_size: Size of embedding vectors\n",
        "        hidden_size: Size of LSTM hidden states\n",
        "        num_layers: Number of LSTM layers\n",
        "        dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Custom LSTM layer\n",
        "        self.lstm = CustomLSTM(embed_size, hidden_size, num_layers)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        Forward pass through encoder.\n",
        "        \n",
        "        Args:\n",
        "            src: Source sequences (batch_size, seq_len)\n",
        "            \n",
        "        Returns:\n",
        "            outputs: All encoder hidden states (batch_size, seq_len, hidden_size)\n",
        "            hidden: Final hidden state (num_layers, batch_size, hidden_size)\n",
        "            cell: Final cell state (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        # Embed source sequences\n",
        "        embedded = self.dropout(self.embedding(src))  # (batch_size, seq_len, embed_size)\n",
        "        \n",
        "        # Pass through LSTM\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1eB3WpU5n4kA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ATTENTION MECHANISM (Bahdanau Attention)\n",
        "# ============================================================================\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau (additive) attention mechanism.\n",
        "    Computes attention weights to focus on relevant parts of encoder outputs.\n",
        "    \n",
        "    Args:\n",
        "        hidden_size: Size of hidden states\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Learnable weight matrices\n",
        "        self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)  # For encoder outputs\n",
        "        self.U_a = nn.Linear(hidden_size, hidden_size, bias=False)  # For decoder hidden state\n",
        "        self.v_a = nn.Linear(hidden_size, 1, bias=False)  # To compute scalar score\n",
        "        \n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Compute attention weights and context vector.\n",
        "        \n",
        "        Args:\n",
        "            decoder_hidden: Current decoder hidden state (batch_size, hidden_size)\n",
        "            encoder_outputs: All encoder hidden states (batch_size, src_len, hidden_size)\n",
        "            \n",
        "        Returns:\n",
        "            context: Context vector (batch_size, hidden_size)\n",
        "            attention_weights: Attention weights (batch_size, src_len)\n",
        "        \"\"\"\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        \n",
        "        # Expand decoder hidden to match encoder outputs shape\n",
        "        # (batch_size, hidden_size) -> (batch_size, src_len, hidden_size)\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        # Compute attention scores (energy)\n",
        "        # score = v_a^T * tanh(W_a * encoder_outputs + U_a * decoder_hidden)\n",
        "        energy = torch.tanh(\n",
        "            self.W_a(encoder_outputs) + self.U_a(decoder_hidden)\n",
        "        )  # (batch_size, src_len, hidden_size)\n",
        "        \n",
        "        attention_scores = self.v_a(energy).squeeze(2)  # (batch_size, src_len)\n",
        "        \n",
        "        # Compute attention weights using softmax\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, src_len)\n",
        "        \n",
        "        # Compute context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1), encoder_outputs\n",
        "        ).squeeze(1)  # (batch_size, hidden_size)\n",
        "        \n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hDKwKh_9n7LV"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DECODER with Custom LSTM and Attention\n",
        "# ============================================================================\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder that uses custom LSTM with attention mechanism.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of target vocabulary\n",
        "        embed_size: Size of embedding vectors\n",
        "        hidden_size: Size of LSTM hidden states\n",
        "        num_layers: Number of LSTM layers\n",
        "        dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        \n",
        "        # Custom LSTM layer (input = embedding + context)\n",
        "        self.lstm = CustomLSTM(embed_size + hidden_size, hidden_size, num_layers)\n",
        "        \n",
        "        # Output projection layer (hidden + context -> vocab)\n",
        "        self.fc_out = nn.Linear(hidden_size * 2, vocab_size)\n",
        "        \n",
        "    def forward(self, target, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass through decoder for one time step.\n",
        "        \n",
        "        Args:\n",
        "            target: Target token (batch_size)\n",
        "            hidden: Previous hidden state (num_layers, batch_size, hidden_size)\n",
        "            cell: Previous cell state (num_layers, batch_size, hidden_size)\n",
        "            encoder_outputs: All encoder hidden states (batch_size, src_len, hidden_size)\n",
        "            \n",
        "        Returns:\n",
        "            output: Prediction logits (batch_size, vocab_size)\n",
        "            hidden: New hidden state\n",
        "            cell: New cell state\n",
        "            attention_weights: Attention weights for visualization\n",
        "        \"\"\"\n",
        "        # Embed target token\n",
        "        target = target.unsqueeze(1)  # (batch_size, 1)\n",
        "        embedded = self.dropout(self.embedding(target))  # (batch_size, 1, embed_size)\n",
        "        \n",
        "        # Use last layer's hidden state for attention\n",
        "        decoder_hidden = hidden[-1]  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Compute attention context\n",
        "        context, attention_weights = self.attention(decoder_hidden, encoder_outputs)\n",
        "        \n",
        "        # Concatenate embedding and context\n",
        "        context = context.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "        lstm_input = torch.cat([embedded, context], dim=2)  # (batch_size, 1, embed_size + hidden_size)\n",
        "        \n",
        "        # Pass through LSTM\n",
        "        lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # lstm_output: (batch_size, 1, hidden_size)\n",
        "        \n",
        "        # Concatenate LSTM output with context for prediction\n",
        "        lstm_output = lstm_output.squeeze(1)  # (batch_size, hidden_size)\n",
        "        context = context.squeeze(1)  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Generate output logits\n",
        "        output = self.fc_out(torch.cat([lstm_output, context], dim=1))  # (batch_size, vocab_size)\n",
        "        \n",
        "        return output, hidden, cell, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w9v1Dz1Mn-gy"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SEQ2SEQ MODEL (Combining Encoder and Decoder)\n",
        "# ============================================================================\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequence-to-Sequence model with attention for machine translation.\n",
        "    \n",
        "    Args:\n",
        "        encoder: Encoder module\n",
        "        decoder: Decoder module\n",
        "        device: Device to run on (CPU or CUDA)\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass through the entire model.\n",
        "        \n",
        "        Args:\n",
        "            src: Source sequences (batch_size, src_len)\n",
        "            trg: Target sequences (batch_size, trg_len)\n",
        "            teacher_forcing_ratio: Probability of using teacher forcing\n",
        "            \n",
        "        Returns:\n",
        "            outputs: Prediction logits (batch_size, trg_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.vocab_size\n",
        "        \n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # Encode source sequence\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # First input to decoder is <sos> token\n",
        "        decoder_input = trg[:, 0]\n",
        "        \n",
        "        # Decode sequence one token at a time\n",
        "        for t in range(1, trg_len):\n",
        "            # Forward pass through decoder\n",
        "            output, hidden, cell, _ = self.decoder(\n",
        "                decoder_input, hidden, cell, encoder_outputs\n",
        "            )\n",
        "            \n",
        "            # Store prediction\n",
        "            outputs[:, t, :] = output\n",
        "            \n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # Get the predicted token\n",
        "            top1 = output.argmax(1)\n",
        "            \n",
        "            # Next input: ground truth if teacher forcing, else predicted token\n",
        "            decoder_input = trg[:, t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVqBN40MoAyZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_prepare_data(data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Load Urdu and English parallel data, combine dev and devtest files,\n",
        "    shuffle, and split into train (70%), validation (15%), and test (15%).\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing data files (default: current directory)\n",
        "    \n",
        "    Returns:\n",
        "        Preprocessed data splits and vocabularies\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    \n",
        "    # Load Urdu (source) data\n",
        "    with open(os.path.join(data_dir, \"urd_Arab.dev\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        urdu_dev = [line.strip() for line in f.readlines()]\n",
        "    with open(os.path.join(data_dir, \"urd_Arab.devtest\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        urdu_devtest = [line.strip() for line in f.readlines()]\n",
        "    \n",
        "    # Load English (target) data\n",
        "    with open(os.path.join(data_dir, \"eng_Latn.dev\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        eng_dev = [line.strip() for line in f.readlines()]\n",
        "    with open(os.path.join(data_dir, \"eng_Latn.devtest\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        eng_devtest = [line.strip() for line in f.readlines()]\n",
        "    \n",
        "    # Combine dev and devtest\n",
        "    urdu_sentences = urdu_dev + urdu_devtest\n",
        "    eng_sentences = eng_dev + eng_devtest\n",
        "    \n",
        "    print(f\"Total sentences: {len(urdu_sentences)}\")\n",
        "    \n",
        "    # Combine and shuffle\n",
        "    combined = list(zip(urdu_sentences, eng_sentences))\n",
        "    random.seed(42)\n",
        "    random.shuffle(combined)\n",
        "    urdu_sentences, eng_sentences = zip(*combined)\n",
        "    \n",
        "    # Split: 70% train, 15% val, 15% test\n",
        "    train_src, temp_src, train_tgt, temp_tgt = train_test_split(\n",
        "        urdu_sentences, eng_sentences, test_size=0.3, random_state=42\n",
        "    )\n",
        "    val_src, test_src, val_tgt, test_tgt = train_test_split(\n",
        "        temp_src, temp_tgt, test_size=0.5, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"Train: {len(train_src)}, Val: {len(val_src)}, Test: {len(test_src)}\")\n",
        "    \n",
        "    return (train_src, train_tgt, val_src, val_tgt, test_src, test_tgt,\n",
        "            urdu_sentences, eng_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hU7KqhE3oFHQ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VOCABULARY BUILDING\n",
        "# ============================================================================\n",
        "\n",
        "def build_vocab(sentences, min_freq=2):\n",
        "    \"\"\"\n",
        "    Build vocabulary from sentences with special tokens.\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of sentences\n",
        "        min_freq: Minimum frequency for a word to be included\n",
        "        \n",
        "    Returns:\n",
        "        vocab: Dictionary mapping tokens to indices\n",
        "        idx_to_token: Dictionary mapping indices to tokens\n",
        "    \"\"\"\n",
        "    # Special tokens\n",
        "    special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "    \n",
        "    # Count word frequencies\n",
        "    word_freq = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    \n",
        "    # Filter by minimum frequency\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "    idx = len(vocab)\n",
        "    \n",
        "    for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    \n",
        "    idx_to_token = {idx: token for token, idx in vocab.items()}\n",
        "    \n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "    \n",
        "    return vocab, idx_to_token\n",
        "\n",
        "\n",
        "def tokenize_sentence(sentence, vocab, max_len):\n",
        "    \"\"\"\n",
        "    Convert sentence to token indices with padding.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Input sentence string\n",
        "        vocab: Vocabulary dictionary\n",
        "        max_len: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        List of token indices\n",
        "    \"\"\"\n",
        "    tokens = [vocab.get(word, vocab[\"<unk>\"]) for word in sentence.split()]\n",
        "    \n",
        "    # Truncate if too long\n",
        "    if len(tokens) > max_len:\n",
        "        tokens = tokens[:max_len]\n",
        "    \n",
        "    # Pad if too short\n",
        "    while len(tokens) < max_len:\n",
        "        tokens.append(vocab[\"<pad>\"])\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "def prepare_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len_src, max_len_tgt):\n",
        "    \"\"\"\n",
        "    Prepare data by tokenizing and converting to tensors.\n",
        "    \n",
        "    Args:\n",
        "        src_sentences: Source sentences\n",
        "        tgt_sentences: Target sentences\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        max_len_src: Max source sequence length\n",
        "        max_len_tgt: Max target sequence length\n",
        "        \n",
        "    Returns:\n",
        "        src_tensor: Source sequences tensor\n",
        "        tgt_tensor: Target sequences tensor (with <sos> and <eos>)\n",
        "    \"\"\"\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    \n",
        "    for src, tgt in zip(src_sentences, tgt_sentences):\n",
        "        # Tokenize source\n",
        "        src_tokens = tokenize_sentence(src, src_vocab, max_len_src)\n",
        "        \n",
        "        # Add <sos> and <eos> to target\n",
        "        tgt_with_special = f\"<sos> {tgt} <eos>\"\n",
        "        tgt_tokens = tokenize_sentence(tgt_with_special, tgt_vocab, max_len_tgt)\n",
        "        \n",
        "        src_data.append(src_tokens)\n",
        "        tgt_data.append(tgt_tokens)\n",
        "    \n",
        "    src_tensor = torch.tensor(src_data, dtype=torch.long)\n",
        "    tgt_tensor = torch.tensor(tgt_data, dtype=torch.long)\n",
        "    \n",
        "    return src_tensor, tgt_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fSDXgzC7oJSj"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for machine translation.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_tensor, tgt_tensor):\n",
        "        self.src_tensor = src_tensor\n",
        "        self.tgt_tensor = tgt_tensor\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.src_tensor)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_tensor[idx], self.tgt_tensor[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcyFbF1WoLsM"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BLEU SCORE CALCULATION\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_bleu_score(model, data_loader, src_vocab, tgt_vocab, device, data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score using Moses multi-bleu.perl script.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        data_loader: DataLoader for evaluation\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        device: Device to run on\n",
        "        data_dir: Directory for output files and perl script\n",
        "        \n",
        "    Returns:\n",
        "        bleu_score: BLEU score as float\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    references = []\n",
        "    \n",
        "    idx_to_tgt = {idx: token for token, idx in tgt_vocab.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for src, tgt in data_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            \n",
        "            # Generate predictions (no teacher forcing)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            output = output.argmax(2)  # Get predicted token indices\n",
        "            \n",
        "            # Convert to text (skip <pad>, <sos>, <eos>)\n",
        "            for pred, ref in zip(output, tgt):\n",
        "                pred_text = []\n",
        "                ref_text = []\n",
        "                \n",
        "                for idx in pred:\n",
        "                    token = idx_to_tgt.get(idx.item(), \"<unk>\")\n",
        "                    if token not in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
        "                        pred_text.append(token)\n",
        "                \n",
        "                for idx in ref:\n",
        "                    token = idx_to_tgt.get(idx.item(), \"<unk>\")\n",
        "                    if token not in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
        "                        ref_text.append(token)\n",
        "                \n",
        "                predictions.append(\" \".join(pred_text))\n",
        "                references.append(\" \".join(ref_text))\n",
        "    \n",
        "    # Save to files for multi-bleu.perl\n",
        "    pred_file = os.path.join(data_dir, \"predictions.txt\")\n",
        "    ref_file = os.path.join(data_dir, \"references.txt\")\n",
        "    \n",
        "    with open(pred_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for pred in predictions:\n",
        "            f.write(pred + \"\\n\")\n",
        "    \n",
        "    with open(ref_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ref in references:\n",
        "            f.write(ref + \"\\n\")\n",
        "    \n",
        "    # Run multi-bleu.perl script\n",
        "    try:\n",
        "        bleu_script = os.path.join(data_dir, \"multi-bleu.perl\")\n",
        "        result = subprocess.run(\n",
        "            [\"perl\", bleu_script, ref_file],\n",
        "            stdin=open(pred_file, \"r\", encoding=\"utf-8\"),\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        bleu_output = result.stdout.strip()\n",
        "        print(f\"BLEU: {bleu_output}\")\n",
        "        \n",
        "        # Extract BLEU score\n",
        "        if \"BLEU = \" in bleu_output:\n",
        "            bleu_score = float(bleu_output.split(\"BLEU = \")[1].split(\",\")[0])\n",
        "        else:\n",
        "            bleu_score = 0.0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BLEU: {e}\")\n",
        "        bleu_score = 0.0\n",
        "    \n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh8xDTLUoOVc"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAINING FUNCTION with BLEU Score Tracking\n",
        "# ============================================================================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs, \n",
        "                src_vocab, tgt_vocab, data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Train the model and track loss and BLEU scores.\n",
        "    \n",
        "    Args:\n",
        "        model: Seq2Seq model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        optimizer: Optimizer\n",
        "        criterion: Loss function\n",
        "        device: Device to run on\n",
        "        epochs: Number of epochs\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        data_dir: Directory for saving files\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing training history\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_bleu_scores = []\n",
        "    val_bleu_scores = []\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        for src, tgt in train_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0.5)\n",
        "            \n",
        "            # Reshape for loss calculation (ignore first token <sos>)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(output, tgt)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "        \n",
        "        # Average training loss\n",
        "        avg_train_loss = epoch_loss / batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "                \n",
        "                # Forward pass (no teacher forcing)\n",
        "                output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "                \n",
        "                # Reshape for loss calculation\n",
        "                output_dim = output.shape[-1]\n",
        "                output = output[:, 1:].reshape(-1, output_dim)\n",
        "                tgt = tgt[:, 1:].reshape(-1)\n",
        "                \n",
        "                # Calculate loss\n",
        "                loss = criterion(output, tgt)\n",
        "                val_loss += loss.item()\n",
        "                batch_count += 1\n",
        "        \n",
        "        # Average validation loss\n",
        "        avg_val_loss = val_loss / batch_count\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Calculate BLEU scores (every few epochs to save time)\n",
        "        if (epoch + 1) % 2 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "            print(f\"Calculating BLEU scores for epoch {epoch + 1}...\")\n",
        "            train_bleu = calculate_bleu_score(model, train_loader, src_vocab, tgt_vocab, device, data_dir)\n",
        "            val_bleu = calculate_bleu_score(model, val_loader, src_vocab, tgt_vocab, device, data_dir)\n",
        "            train_bleu_scores.append((epoch + 1, train_bleu))\n",
        "            val_bleu_scores.append((epoch + 1, val_bleu))\n",
        "        else:\n",
        "            # Use previous BLEU scores\n",
        "            if train_bleu_scores:\n",
        "                train_bleu = train_bleu_scores[-1][1]\n",
        "                val_bleu = val_bleu_scores[-1][1]\n",
        "            else:\n",
        "                train_bleu, val_bleu = 0, 0\n",
        "        \n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  Train BLEU: {train_bleu:.2f} | Val BLEU: {val_bleu:.2f}\")\n",
        "        print(\"-\" * 70)\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_bleu_scores': train_bleu_scores,\n",
        "        'val_bleu_scores': val_bleu_scores\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsIDEVXXoQ-t"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EVALUATION AND VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_curves(history, data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Plot loss and BLEU score curves for training and validation.\n",
        "    \n",
        "    Args:\n",
        "        history: Dictionary containing training history\n",
        "        data_dir: Directory to save the plot\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot loss curves\n",
        "    epochs = range(1, len(history['train_losses']) + 1)\n",
        "    ax1.plot(epochs, history['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot BLEU score curves\n",
        "    train_epochs, train_bleu = zip(*history['train_bleu_scores'])\n",
        "    val_epochs, val_bleu = zip(*history['val_bleu_scores'])\n",
        "    \n",
        "    ax2.plot(train_epochs, train_bleu, 'b-o', label='Train BLEU', linewidth=2, markersize=6)\n",
        "    ax2.plot(val_epochs, val_bleu, 'r-o', label='Validation BLEU', linewidth=2, markersize=6)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('BLEU Score', fontsize=12)\n",
        "    ax2.set_title('Training and Validation BLEU Score', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(data_dir, 'training_curves.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Training curves saved as '{plot_path}'\")\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, model, src_vocab, tgt_vocab, device, max_len=50):\n",
        "    \"\"\"\n",
        "    Translate a single sentence from Urdu to English.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Urdu sentence to translate\n",
        "        model: Trained model\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        device: Device to run on\n",
        "        max_len: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        Translated English sentence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    idx_to_tgt = {idx: token for token, idx in tgt_vocab.items()}\n",
        "    \n",
        "    # Tokenize source sentence\n",
        "    tokens = tokenize_sentence(sentence, src_vocab, max_len)\n",
        "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Create dummy target (just <sos> token repeated)\n",
        "    tgt_tensor = torch.tensor([[tgt_vocab['<sos>']] * max_len], dtype=torch.long).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get encoder outputs\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "        \n",
        "        # Start with <sos> token\n",
        "        decoder_input = torch.tensor([tgt_vocab['<sos>']], dtype=torch.long).to(device)\n",
        "        \n",
        "        translated_tokens = []\n",
        "        \n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell, _ = model.decoder(decoder_input, hidden, cell, encoder_outputs)\n",
        "            \n",
        "            # Get predicted token\n",
        "            predicted_token = output.argmax(1).item()\n",
        "            \n",
        "            # Stop if <eos> token is predicted\n",
        "            if predicted_token == tgt_vocab['<eos>']:\n",
        "                break\n",
        "            \n",
        "            # Add token to translation\n",
        "            token_str = idx_to_tgt.get(predicted_token, '<unk>')\n",
        "            if token_str not in ['<pad>', '<sos>']:\n",
        "                translated_tokens.append(token_str)\n",
        "            \n",
        "            # Use predicted token as next input\n",
        "            decoder_input = torch.tensor([predicted_token], dtype=torch.long).to(device)\n",
        "    \n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "\n",
        "def evaluate_test_set(model, test_loader, src_vocab, tgt_vocab, device, data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set and print final BLEU score.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_loader: Test data loader\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        device: Device to run on\n",
        "        data_dir: Directory for saving files\n",
        "        \n",
        "    Returns:\n",
        "        test_bleu: BLEU score on test set\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EVALUATING ON TEST SET\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    test_bleu = calculate_bleu_score(model, test_loader, src_vocab, tgt_vocab, device, data_dir)\n",
        "    \n",
        "    print(f\"\\nFinal Test BLEU Score: {test_bleu:.2f}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return test_bleu\n",
        "\n",
        "\n",
        "def create_results_table(history, test_bleu, data_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Create and display comparative results table.\n",
        "    \n",
        "    Args:\n",
        "        history: Training history dictionary\n",
        "        test_bleu: Test set BLEU score\n",
        "        data_dir: Directory to save the table\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"COMPARATIVE RESULTS TABLE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Dataset':<20} {'Final Loss':<15} {'Final BLEU Score':<20}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Get final values\n",
        "    final_train_loss = history['train_losses'][-1]\n",
        "    final_val_loss = history['val_losses'][-1]\n",
        "    final_train_bleu = history['train_bleu_scores'][-1][1]\n",
        "    final_val_bleu = history['val_bleu_scores'][-1][1]\n",
        "    \n",
        "    print(f\"{'Training Set':<20} {final_train_loss:<15.4f} {final_train_bleu:<20.2f}\")\n",
        "    print(f\"{'Validation Set':<20} {final_val_loss:<15.4f} {final_val_bleu:<20.2f}\")\n",
        "    print(f\"{'Test Set':<20} {'-':<15} {test_bleu:<20.2f}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Save to file\n",
        "    table_path = os.path.join(data_dir, \"results_table.txt\")\n",
        "    with open(table_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"COMPARATIVE RESULTS TABLE\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(f\"{'Dataset':<20} {'Final Loss':<15} {'Final BLEU Score':<20}\\n\")\n",
        "        f.write(\"-\" * 70 + \"\\n\")\n",
        "        f.write(f\"{'Training Set':<20} {final_train_loss:<15.4f} {final_train_bleu:<20.2f}\\n\")\n",
        "        f.write(f\"{'Validation Set':<20} {final_val_loss:<15.4f} {final_val_bleu:<20.2f}\\n\")\n",
        "        f.write(f\"{'Test Set':<20} {'-':<15} {test_bleu:<20.2f}\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "    \n",
        "    print(f\"Results table saved to '{table_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ij72L1dUoZlZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Platform: linux\n",
            "Current directory: /content\n",
            "Available paths:\n",
            "  /mnt/ exists: True\n",
            "  /mnt/c/ exists: False\n",
            "\n",
            "Files: []\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: Load and Prepare Data\n",
        "# ============================================================================\n",
        "\n",
        "# Load data\n",
        "(train_src, train_tgt, val_src, val_tgt, test_src, test_tgt,\n",
        " all_urdu, all_eng) = load_and_prepare_data()\n",
        "\n",
        "# Build vocabularies\n",
        "print(\"\\nBuilding vocabularies...\")\n",
        "urdu_vocab, urdu_idx_to_token = build_vocab(all_urdu, min_freq=2)\n",
        "eng_vocab, eng_idx_to_token = build_vocab(all_eng, min_freq=2)\n",
        "\n",
        "print(f\"Urdu vocab size: {len(urdu_vocab)}\")\n",
        "print(f\"English vocab size: {len(eng_vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qCkV19soeaa"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: Prepare Tensors and DataLoaders\n",
        "# ============================================================================\n",
        "\n",
        "# Define maximum sequence lengths\n",
        "MAX_LEN_SRC = 60  # Maximum length for Urdu sentences\n",
        "MAX_LEN_TGT = 60  # Maximum length for English sentences (with <sos> and <eos>)\n",
        "\n",
        "print(\"\\nPreparing data tensors...\")\n",
        "\n",
        "# Prepare train, validation, and test data\n",
        "train_src_tensor, train_tgt_tensor = prepare_data(\n",
        "    train_src, train_tgt, urdu_vocab, eng_vocab, MAX_LEN_SRC, MAX_LEN_TGT\n",
        ")\n",
        "val_src_tensor, val_tgt_tensor = prepare_data(\n",
        "    val_src, val_tgt, urdu_vocab, eng_vocab, MAX_LEN_SRC, MAX_LEN_TGT\n",
        ")\n",
        "test_src_tensor, test_tgt_tensor = prepare_data(\n",
        "    test_src, test_tgt, urdu_vocab, eng_vocab, MAX_LEN_SRC, MAX_LEN_TGT\n",
        ")\n",
        "\n",
        "print(f\"Train data shape: {train_src_tensor.shape}\")\n",
        "print(f\"Val data shape: {val_src_tensor.shape}\")\n",
        "print(f\"Test data shape: {test_src_tensor.shape}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TranslationDataset(train_src_tensor, train_tgt_tensor)\n",
        "val_dataset = TranslationDataset(val_src_tensor, val_tgt_tensor)\n",
        "test_dataset = TranslationDataset(test_src_tensor, test_tgt_tensor)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nNumber of batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BpPvrjQoidg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: Initialize Model\n",
        "# ============================================================================\n",
        "\n",
        "# Hyperparameters\n",
        "EMBED_SIZE = 256      # Embedding dimension\n",
        "HIDDEN_SIZE = 512     # LSTM hidden state size\n",
        "NUM_LAYERS = 2        # Number of LSTM layers (stacked)\n",
        "DROPOUT = 0.3         # Dropout probability\n",
        "LEARNING_RATE = 0.001 # Learning rate\n",
        "EPOCHS = 15           # Number of training epochs\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {DEVICE}\")\n",
        "\n",
        "# Get vocabulary sizes\n",
        "INPUT_DIM = len(urdu_vocab)\n",
        "OUTPUT_DIM = len(eng_vocab)\n",
        "\n",
        "print(f\"\\nModel Configuration:\")\n",
        "print(f\"  Input Vocab Size: {INPUT_DIM}\")\n",
        "print(f\"  Output Vocab Size: {OUTPUT_DIM}\")\n",
        "print(f\"  Embedding Size: {EMBED_SIZE}\")\n",
        "print(f\"  Hidden Size: {HIDDEN_SIZE}\")\n",
        "print(f\"  Number of Layers: {NUM_LAYERS}\")\n",
        "print(f\"  Dropout: {DROPOUT}\")\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "encoder = Encoder(INPUT_DIM, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "\n",
        "# Initialize Seq2Seq model\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"  Total Parameters: {count_parameters(model):,}\")\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=eng_vocab['<pad>'])  # Ignore padding in loss\n",
        "\n",
        "print(\"\\nModel initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXMDtqSuon-O",
        "outputId": "f8a45197-0c7e-40d9-d7fb-41a3e6bf1107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 7.8599, Val Loss: 7.7435\n",
            "Epoch 2, Train Loss: 7.0082, Val Loss: 8.0348\n",
            "Epoch 3, Train Loss: 6.5884, Val Loss: 8.2219\n",
            "Epoch 4, Train Loss: 5.9456, Val Loss: 8.4480\n",
            "Epoch 5, Train Loss: 5.1814, Val Loss: 8.7294\n",
            "Epoch 6, Train Loss: 4.2468, Val Loss: 8.9525\n",
            "Epoch 7, Train Loss: 3.6056, Val Loss: 9.1630\n",
            "Epoch 8, Train Loss: 3.0433, Val Loss: 9.4528\n",
            "Epoch 9, Train Loss: 2.6283, Val Loss: 9.5557\n",
            "Epoch 10, Train Loss: 2.2081, Val Loss: 9.6552\n",
            "Epoch 11, Train Loss: 1.8649, Val Loss: 9.7767\n",
            "Epoch 12, Train Loss: 1.3366, Val Loss: 10.0486\n",
            "Epoch 13, Train Loss: 1.0163, Val Loss: 10.1662\n",
            "Epoch 14, Train Loss: 0.6626, Val Loss: 10.3592\n",
            "Epoch 15, Train Loss: 0.4294, Val Loss: 10.5017\n",
            "Epoch 16, Train Loss: 0.3011, Val Loss: 10.5552\n",
            "Epoch 17, Train Loss: 0.2067, Val Loss: 10.8017\n",
            "Epoch 18, Train Loss: 0.1367, Val Loss: 10.9083\n",
            "Epoch 19, Train Loss: 0.0955, Val Loss: 10.9571\n",
            "Epoch 20, Train Loss: 0.0672, Val Loss: 11.0926\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: Train the Model\n",
        "# ============================================================================\n",
        "\n",
        "# Train the model\n",
        "history = train_model(\n",
        "    model, train_loader, val_loader, optimizer, criterion, \n",
        "    DEVICE, EPOCHS, urdu_vocab, eng_vocab\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = 'urdu_to_english_lstm_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"\\nModel saved as '{model_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "6fut40KZot9r",
        "outputId": "197d35f7-832f-4860-a4c8-a8a77d3525f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: Plot Training Curves\n",
        "# ============================================================================\n",
        "\n",
        "plot_training_curves(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5HJV6jzox8t"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: Evaluate on Test Set (Run ONCE)\n",
        "# ============================================================================\n",
        "\n",
        "test_bleu = evaluate_test_set(model, test_loader, urdu_vocab, eng_vocab, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf9ZCHYso3_-",
        "outputId": "3cf83ca3-f823-469c-ed6e-514de140a3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Predictions and References:\n",
            "Prediction: The gas bug of the race and the first person of the present and the first of money, of the Riga Peace the first of the Riga Peace the race\n",
            "Reference: Lodin also said officials decided to cancel the runoff in order to save Afghans the expense and security risk of another election.\n",
            "Prediction: As a result, the city of the and World to be the traditional to be the place of a \"thorough and in the\n",
            "Reference: Stone steps are laid along most of the path, and in the steeper sections steel cables provide a supporting handrail.\n",
            "Prediction: No one of the University in the day that he was not possible that he will be issued by a new in the same way to launch a strong\n",
            "Reference: Even traditionally, though, not all SÃ¡mi have been involved in big scale reindeer husbandry, but lived from fishing, hunting and similar, having reindeer mostly as draft animals.\n",
            "Prediction: The case was also in the 2008 of the system and are also to see the of the love of the to the of the the to the of the\n",
            "Reference: Others include Livingston Island, and Deception where the flooded caldera of a still-active volcano provides a spectacular natural harbour.\n",
            "Prediction: Neurobiological data provide physical evidence for a theoretical approach to the investigation of cognition. Therefore it narrows the research area and makes it much more exact.\n",
            "Reference: Each temple had an open temple courtyard and then an inner sanctuary that only the priests could enter.\n",
            "BLEU Score: BLEU = 0.00, 16.0/1.5/0.1/0.0 (BP=1.000, ratio=1.044, hyp_len=6771, ref_len=6486)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: Create Results Table\n",
        "# ============================================================================\n",
        "\n",
        "create_results_table(history, test_bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn_K5Hluo5-L",
        "outputId": "624f0234-18b2-4dda-937a-664ed2c8a8b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: Test Translation on Sample Sentences\n",
        "# ============================================================================\n",
        "\n",
        "# Sample Urdu sentences for testing\n",
        "sample_sentences = [\n",
        "    \"Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’\",  # What is your name?\n",
        "    \"Ù…ÛŒÚº Ù¹Ú¾ÛŒÚ© ÛÙˆÚº\",      # I am fine\n",
        "    \"Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø§Ú†Ú¾Ø§ ÛÛ’\",   # The weather is good today\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAMPLE TRANSLATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for urdu_sent in sample_sentences:\n",
        "    english_translation = translate_sentence(urdu_sent, model, urdu_vocab, eng_vocab, DEVICE)\n",
        "    print(f\"\\nUrdu:    {urdu_sent}\")\n",
        "    print(f\"English: {english_translation}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0nHczKP16gt",
        "outputId": "f6e387c3-e506-47a4-f8f8-b16c121a9d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated: The balance of an organization the US <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> customer's need. <eos> <eos> <eos> <eos> <eos> <eos> customer's need.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# BONUS: Model Architecture Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nCustom LSTM Implementation:\")\n",
        "print(\"  âœ“ Custom LSTM Cell with 4 gates (input, forget, cell, output)\")\n",
        "print(\"  âœ“ No usage of nn.LSTM or nn.RNN library\")\n",
        "print(\"  âœ“ Manual implementation of forward pass through LSTM cells\")\n",
        "print(\"\\nEncoder:\")\n",
        "print(f\"  - Embedding Layer: {INPUT_DIM} â†’ {EMBED_SIZE}\")\n",
        "print(f\"  - Custom LSTM Layers: {NUM_LAYERS} layers, hidden size {HIDDEN_SIZE}\")\n",
        "print(\"\\nDecoder:\")\n",
        "print(f\"  - Embedding Layer: {OUTPUT_DIM} â†’ {EMBED_SIZE}\")\n",
        "print(f\"  - Bahdanau Attention Mechanism\")\n",
        "print(f\"  - Custom LSTM Layers: {NUM_LAYERS} layers, hidden size {HIDDEN_SIZE}\")\n",
        "print(f\"  - Output Layer: {HIDDEN_SIZE * 2} â†’ {OUTPUT_DIM}\")\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(f\"  - Optimizer: Adam (lr={LEARNING_RATE})\")\n",
        "print(f\"  - Loss Function: CrossEntropyLoss (ignoring padding)\")\n",
        "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"  - Teacher Forcing Ratio: 0.5\")\n",
        "print(\"\\nEvaluation:\")\n",
        "print(f\"  - BLEU Score calculated using Moses multi-bleu.perl\")\n",
        "print(f\"  - Evaluation on Train, Validation, and Test sets\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
